 reduces VRAM usage by over 50% when processing long-context inputs of 25,000 tokens. With optimized settings, memory consumption drops from 60.2 GB (7B model) to just 28.2 GB (3B model), enabling deployment on 24GB GPUs commonly found in high-end desktops and laptop computers â€” instead of the larger dedicated GPU clusters or workstations found in enterprises. According to the developers, it achieves this through architectural features such as the Thinker-Talker design and a custom position embe