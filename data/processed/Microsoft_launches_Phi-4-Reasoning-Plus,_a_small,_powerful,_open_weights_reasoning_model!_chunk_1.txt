or tasks requiring deep, structured reasoning. Building on the architecture of the previously released Phi-4, the new model integrates supervised fine-tuning and reinforcement learning to deliver improved performance on benchmarks in mathematics, science, coding, and logic-based tasks. Phi-4-reasoning-plus is a 14-billion parameter dense decoder-only Transformer model that emphasizes quality over scale. Its training process involved 16 billion tokens—about 8.3 billion of them unique—drawn from s