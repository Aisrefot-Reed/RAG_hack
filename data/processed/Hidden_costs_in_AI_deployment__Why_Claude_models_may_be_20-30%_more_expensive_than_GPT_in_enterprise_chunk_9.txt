 Implementation of tokenizers GPT models use Byte Pair Encoding (BPE), which merges frequently co-occurring character pairs to form tokens. Specifically, the latest GPT models use the open-source o200k_base tokenizer. The actual tokens used by GPT-4o (in the tiktoken tokenizer) can be viewed here. JSON { #reasoning "o1-xxx": "o200k_base", "o3-xxx": "o200k_base", # chat "chatgpt-4o-": "o200k_base", "gpt-4o-xxx": "o200k_base", # e.g., gpt-4o-2024-05-13 "gpt-4-xxx": "cl100k_base", # e.g., gpt-4-031